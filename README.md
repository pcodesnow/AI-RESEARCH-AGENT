# AI-RESEARCH-AGENT

#
# This Python script demonstrates a RAG (Retrieval-Augmented Generation) workflow
# for a Research Agent. The agent can take a research question, find relevant
# papers from a knowledge base, and then use that information to generate a
# detailed response.
#
# A real-world application would use:
# 1. A vector database for efficient searching of millions of documents.
# 2. A more robust document loader for handling PDF files of full research papers.
# 3. An actual API call to the IBM Granite model hosted on watsonx.ai.
#
import numpy as np
from typing import List, Dict
from sklearn.metrics.pairwise import cosine_similarity

# --- Configuration and IBM Cloud Credentials ---
# Replace this with your actual IBM watsonx.ai API key and endpoint.
# The API key is obtained from IBM Cloud IAM.
API_KEY = "YOUR_IBM_WATSONX_API_KEY"
ENDPOINT = "https://us-south.ml.cloud.ibm.com/..." # Your watsonx.ai endpoint
GRANITE_MODEL_ID = "ibm/granite-13b-instruct-v2" # The model ID to use

# --- Mock Data: Simulating a Research Paper Index ---
# This dictionary represents our knowledge base of research paper abstracts.
# In a real application, you would load these from a folder of PDF files.
research_papers = {
    "paper_1": "Title: The Impact of Climate Change on Coral Reefs. Abstract: This paper investigates the long-term effects of ocean acidification and rising sea temperatures on the health and biodiversity of coral reefs in the Pacific Ocean. Data from the last 20 years show a significant decline in coral coverage.",
    "paper_2": "Title: Machine Learning for Protein Folding. Abstract: We present a novel deep learning model that predicts the 3D structure of proteins with high accuracy. The model was trained on a large dataset of known protein structures and outperforms existing computational methods.",
    "paper_3": "Title: A Review of Sustainable Urban Planning. Abstract: This literature review examines key strategies for developing sustainable cities, focusing on public transportation, green infrastructure, and renewable energy integration. Case studies from Copenhagen and Singapore are analyzed.",
    "paper_4": "Title: Renewable Energy Storage Solutions. Abstract: This study explores emerging technologies for grid-scale energy storage, including advanced battery systems and pumped-hydro storage. The findings highlight the critical role of these solutions in the transition to renewable energy sources.",
}

# Simulate embeddings for the research papers. These would be generated by a
# real embedding model like `sentence-transformers`.
# For this example, we use random vectors for demonstration.
mock_embeddings = {
    "paper_1": np.random.rand(1024),
    "paper_2": np.random.rand(1024),
    "paper_3": np.random.rand(1024),
    "paper_4": np.random.rand(1024),
}

# A function to simulate getting an embedding for a new text query.
def get_embedding(text: str) -> np.ndarray:
    """Simulates getting an embedding for a text query."""
    # A real implementation would use a library like 'sentence-transformers'.
    # from sentence_transformers import SentenceTransformer
    # model = SentenceTransformer('all-MiniLM-L6-v2')
    # return model.encode(text)
    return np.random.rand(1024) # Random vector for this demonstration

# --- RAG Retrieval Logic ---
def retrieve_relevant_papers(query: str, embeddings: Dict[str, np.ndarray], top_k: int = 2) -> List[str]:
    """
    Retrieves the most relevant research paper abstracts based on cosine similarity.

    Args:
        query (str): The user's research question.
        embeddings (Dict[str, np.ndarray]): A dictionary of paper IDs to their embeddings.
        top_k (int): The number of top papers to retrieve.

    Returns:
        List[str]: A list of the retrieved paper abstracts.
    """
    query_embedding = get_embedding(query).reshape(1, -1)
    
    # Calculate cosine similarity between the query and all paper embeddings
    similarities = {
        paper_id: cosine_similarity(query_embedding, emb.reshape(1, -1))[0][0]
        for paper_id, emb in embeddings.items()
    }
    
    # Sort by similarity and get the top_k paper IDs
    sorted_papers = sorted(similarities.items(), key=lambda item: item[1], reverse=True)
    top_paper_ids = [paper_id for paper_id, _ in sorted_papers[:top_k]]
    
    # Retrieve the actual text content (abstracts) of the top papers
    retrieved_abstracts = [research_papers[paper_id] for paper_id in top_paper_ids]
    return retrieved_abstracts

# --- IBM Granite Prompt Generation and Response Simulation ---
def generate_research_response(query: str, context: List[str]) -> str:
    """
    Simulates calling the IBM Granite model with the query and retrieved context.
    
    Args:
        query (str): The user's research question.
        context (List[str]): The retrieved paper abstracts.
        
    Returns:
        str: A simulated response from the LLM.
    """
    # Construct a well-engineered prompt. The prompt instructs the model to
    # act as a research assistant and use the provided context to answer.
    prompt = f"""
    You are an AI research assistant. Your task is to provide a detailed and accurate
    response to the research question below, based *only* on the provided context
    from academic papers. Do not use any outside knowledge.
    
    Context from relevant papers:
    ---
    {'\n\n'.join(context)}
    ---
    
    Research Question:
    {query}
    
    Provide a comprehensive answer, including a summary of findings from the
    context, and if possible, suggest a potential hypothesis based on the findings.
    """
    
    # In a real implementation, you would make an API call here.
    # Example using requests (you would need to handle authentication):
    # import requests
    # headers = {"Authorization": f"Bearer {API_KEY}"}
    # payload = {
    #     "model_id": GRANITE_MODEL_ID,
    #     "input": prompt,
    #     "parameters": {"decoding_method": "greedy", "max_new_tokens": 500}
    # }
    # response = requests.post(ENDPOINT, headers=headers, json=payload)
    # return response.json()["results"][0]["generated_text"]
    
    print(f"\n--- Simulating API Call to IBM Granite ---")
    print(f"Prompt sent to model: \n{prompt}")
    print(f"--- End of Prompt ---")

    # For this demonstration, we return a simulated response.
    return "Simulated AI-generated research summary and hypothesis based on the retrieved papers."

# --- Main Application Logic ---
if __name__ == "__main__":
    print("Research Agent is ready. Enter your research question.")
    
    while True:
        user_query = input("\nResearch Question (e.g., 'What are the effects of climate change on ocean ecosystems?'): ")
        if user_query.lower() == 'exit':
            break

        print(f"\nSearching for relevant literature related to: '{user_query}'...")
        
        # 1. Retrieve the most relevant research paper abstracts
        retrieved_papers = retrieve_relevant_papers(user_query, mock_embeddings)
        print("\nRetrieved documents:")
        for paper in retrieved_papers:
            print(f"- {paper}")

        # 2. Use the retrieved content to generate a response with the LLM
        response = generate_research_response(user_query, retrieved_papers)
        
        print("\n------------------------------")
        print("Research Agent Response:")
        print(response)
        print("------------------------------")
